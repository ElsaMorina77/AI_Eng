{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842c4e38-5b03-475e-95a3-b72b357a1e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "import re\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, TrainingArguments, Trainer, pipeline\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datasets \n",
    "import evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1f72d-2beb-42f7-b093-0372d3151835",
   "metadata": {},
   "source": [
    "***PreProcess the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d528fc2-15cf-48b0-823e-db2d6d1e30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('./emotions_data/emotion-labels-train.csv') \n",
    "data_test = pd.read_csv('./emotions_data/emotion-labels-test.csv')\n",
    "data_val = pd.read_csv('./emotions_data/emotion-labels-val.csv')\n",
    "# data should be saved in a folder called 'emotions' which is saved in the same place as your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dafec46-166d-42bb-a7cd-e85565db9be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been waiting all week for this game ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #ch...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Just got back from seeing @GaryDelaney in Burs...   joy\n",
       "1  Oh dear an evening of absolute hilarity I don'...   joy\n",
       "2  Been waiting all week for this game ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #ch...   joy\n",
       "3  @gardiner_love : Thank you so much, Gloria! Yo...   joy\n",
       "4  I feel so blessed to work with the family that...   joy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843f19d1-4cbb-4eac-9054-3de1b9b612d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_train, data_test,data_val], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "763aaf85-a48a-4378-bec6-04f19158345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text'].apply(lambda x: clean(x, no_emoji=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13bc0212-a12b-4f66-8d20-531bf1f415c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text_clean'].apply(lambda x: re.sub('@[^s]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cfc68db-f249-42cc-bc63-d3af56563ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "      <td>just got back from seeing slem. amazing!! face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh dear an evening of absolute hilarity I don'...</td>\n",
       "      <td>joy</td>\n",
       "      <td>oh dear an evening of absolute hilarity i don'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Been waiting all week for this game ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #ch...</td>\n",
       "      <td>joy</td>\n",
       "      <td>been waiting all week for this game #cheer #fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@gardiner_love : Thank you so much, Gloria! Yo...</td>\n",
       "      <td>joy</td>\n",
       "      <td>so much, gloria! you're so sweet, and thoughtf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel so blessed to work with the family that...</td>\n",
       "      <td>joy</td>\n",
       "      <td>i feel so blessed to work with the family that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Today I reached 1000 subscribers on YT!! , #go...</td>\n",
       "      <td>joy</td>\n",
       "      <td>today i reached 1000 subscribers on yt!! , #go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Singaholic121 Good morning, love! Happy first...</td>\n",
       "      <td>joy</td>\n",
       "      <td>@singaholic121 good morning, love! happy first...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#BridgetJonesBaby is the best thing I've seen ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>#bridgetjonesbaby is the best thing i've seen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Just got back from seeing @GaryDelaney in Burs...</td>\n",
       "      <td>joy</td>\n",
       "      <td>just got back from seeing slem. amazing!! face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@IndyMN I thought the holidays could not get a...</td>\n",
       "      <td>joy</td>\n",
       "      <td>s could not get any more cheerful, and then i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I'm just still . So happy .\\nA blast</td>\n",
       "      <td>joy</td>\n",
       "      <td>i'm just still . so happy .\\na blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>It's meant to be!! #happy #happy</td>\n",
       "      <td>joy</td>\n",
       "      <td>it's meant to be!! #happy #happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>üí•‚öñÔ∏èYeah‚ÄºÔ∏è PAUL‚ÄºÔ∏è‚öñÔ∏èüí•  #glorious #BB18</td>\n",
       "      <td>joy</td>\n",
       "      <td>yeah!! paul!! #glorious #bb18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>My morning started off amazing!! Hopefully the...</td>\n",
       "      <td>joy</td>\n",
       "      <td>my morning started off amazing!! hopefully the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>üò± @cailamarsai you've had me üòÇ üòÇ the whole tim...</td>\n",
       "      <td>joy</td>\n",
       "      <td>sai you've had me the whole time watching shab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@iamTinaDatta love you so much #smile üòäüòä</td>\n",
       "      <td>joy</td>\n",
       "      <td>so much #smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@WyoWiseGuy @LivingVertical however, REI did o...</td>\n",
       "      <td>joy</td>\n",
       "      <td>seguy s well. can't believe how exponentially ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2 days until #GoPackGo and 23 days until #GoGi...</td>\n",
       "      <td>joy</td>\n",
       "      <td>2 days until #gopackgo and 23 days until #gogi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@TheMandyMoore You are beyond wonderful.  Your...</td>\n",
       "      <td>joy</td>\n",
       "      <td>singing prowess is phenomenal but damn... i'm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@luckiiCHARM_ Luckii, I'm changing in so many ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>so many ways bc of him!! it's a scary but joyf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text label  \\\n",
       "0   Just got back from seeing @GaryDelaney in Burs...   joy   \n",
       "1   Oh dear an evening of absolute hilarity I don'...   joy   \n",
       "2   Been waiting all week for this game ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è #ch...   joy   \n",
       "3   @gardiner_love : Thank you so much, Gloria! Yo...   joy   \n",
       "4   I feel so blessed to work with the family that...   joy   \n",
       "5   Today I reached 1000 subscribers on YT!! , #go...   joy   \n",
       "6   @Singaholic121 Good morning, love! Happy first...   joy   \n",
       "7   #BridgetJonesBaby is the best thing I've seen ...   joy   \n",
       "8   Just got back from seeing @GaryDelaney in Burs...   joy   \n",
       "9   @IndyMN I thought the holidays could not get a...   joy   \n",
       "10               I'm just still . So happy .\\nA blast   joy   \n",
       "11                   It's meant to be!! #happy #happy   joy   \n",
       "12               üí•‚öñÔ∏èYeah‚ÄºÔ∏è PAUL‚ÄºÔ∏è‚öñÔ∏èüí•  #glorious #BB18   joy   \n",
       "13  My morning started off amazing!! Hopefully the...   joy   \n",
       "14  üò± @cailamarsai you've had me üòÇ üòÇ the whole tim...   joy   \n",
       "15           @iamTinaDatta love you so much #smile üòäüòä   joy   \n",
       "16  @WyoWiseGuy @LivingVertical however, REI did o...   joy   \n",
       "17  2 days until #GoPackGo and 23 days until #GoGi...   joy   \n",
       "18  @TheMandyMoore You are beyond wonderful.  Your...   joy   \n",
       "19  @luckiiCHARM_ Luckii, I'm changing in so many ...   joy   \n",
       "\n",
       "                                           text_clean  \n",
       "0   just got back from seeing slem. amazing!! face...  \n",
       "1   oh dear an evening of absolute hilarity i don'...  \n",
       "2   been waiting all week for this game #cheer #fr...  \n",
       "3   so much, gloria! you're so sweet, and thoughtf...  \n",
       "4   i feel so blessed to work with the family that...  \n",
       "5   today i reached 1000 subscribers on yt!! , #go...  \n",
       "6   @singaholic121 good morning, love! happy first...  \n",
       "7   #bridgetjonesbaby is the best thing i've seen ...  \n",
       "8   just got back from seeing slem. amazing!! face...  \n",
       "9   s could not get any more cheerful, and then i ...  \n",
       "10               i'm just still . so happy .\\na blast  \n",
       "11                   it's meant to be!! #happy #happy  \n",
       "12                      yeah!! paul!! #glorious #bb18  \n",
       "13  my morning started off amazing!! hopefully the...  \n",
       "14  sai you've had me the whole time watching shab...  \n",
       "15                                     so much #smile  \n",
       "16  seguy s well. can't believe how exponentially ...  \n",
       "17  2 days until #gopackgo and 23 days until #gogi...  \n",
       "18  singing prowess is phenomenal but damn... i'm ...  \n",
       "19  so many ways bc of him!! it's a scary but joyf...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0676e06-f7b8-490d-9672-ad01226b7e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['label'].value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "468f01e6-d89b-4dff-8f71-ba7d27338bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jw/c29_qvvs3yb90jqlkdbpt4_m0000gn/T/ipykernel_15618/2135407547.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))\n"
     ]
    }
   ],
   "source": [
    "g = data.groupby('label')\n",
    "data = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "911c615a-4daa-4f48-8216-c20c7198598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6d1690a-43fb-470a-9658-029198ae6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_int'] = LabelEncoder().fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68fd7f72-18d9-4f02-8150-50d73363f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346292ed-c705-4106-aa41-3ce71e6b757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, test_split = train_test_split(data, train_size = 0.8)\n",
    "train_split, val_split = train_test_split(train_split, train_size = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa75f39-5e5d-4f54-bb84-426682bd6d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4414\n",
      "1227\n",
      "491\n"
     ]
    }
   ],
   "source": [
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "print(len(val_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6a31175-6d7a-49c6-8c80-faf64cb2928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    \"label\": train_split.label_int.values,\n",
    "    \"text\": train_split.text_clean.values\n",
    "})\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"label\": test_split.label_int.values,\n",
    "    \"text\": test_split.text_clean.values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a3c955-b6bf-411d-ace3-575b90bee31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = datasets.Dataset.from_dict(train_df)\n",
    "test_df = datasets.Dataset.from_dict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "100dd02f-d740-43c3-9644-1d65f176c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dict = datasets.DatasetDict({'train': train_df, \"test\": test_df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fe4eba7-f642-4570-b4af-0e3a40d1ca36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 4414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1227\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5373c-7e2e-46ea-9a68-03a4d89203b1",
   "metadata": {},
   "source": [
    "Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95c7a90-5d23-4881-8921-789d85596749",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02efd91e-7e23-45c3-8b2c-55089cf1a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding = \"max_length\", max_length = 128, truncation= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57535a55-5d23-4584-af4f-e8a446b8ceb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186dbce30f994129af4f8ca2c47aa0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fa9417b347444cb5c9dc7e212986e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets_dict.map(tokenize_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "078ceef6-61ec-4798-a379-6440cdd85a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4414\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1227\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af52031e-8898-4793-96d9-419bc59a9077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "st! novelists shouldn't be discouraged by rejection, but they usually are because their work is so personal.\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a04735cb-7ec4-4355-b1b1-11423b102b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 17, 639, 136, 16153, 23, 5859, 26, 46, 39, 21359, 37, 12704, 19, 57, 63, 1044, 41, 149, 58, 154, 27, 102, 739, 9, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af968cd6-bdb7-40f0-8ef2-0022a1c68cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b68e056-9918-4ce0-af08-4e43c62b9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['token_type_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ad73b1e-f74a-40be-9bcb-fa41e5fbc085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train']['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b5bdeaa-6494-4faf-9d34-b592c8438cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(100))\n",
    "small_test_dataset = tokenized_datasets['test'].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e67be3-b5cf-4c1e-8fb0-3c7e94781a3f",
   "metadata": {},
   "source": [
    "FINE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1ee08f9-2117-499d-94f6-950fc41763b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased',\n",
    "                                                      num_labels =NUM_LABELS,\n",
    "                                                      id2label = {0: 'anger', 1: 'fear', 2: 'joy', 3: 'sadness'},\n",
    "                                                      use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4d5ea9b-0ed5-420e-80ca-2a1cdb75fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3024e676-79ac-4175-8015-4bbd241b93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis = -1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f19c1015-8891-40d8-b080-46a33e84abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainings_args = TrainingArguments(output_dir='test_trainer', eval_strategy='epoch', num_train_epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f8ef499-44d8-404c-842b-29fef3a8ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=trainings_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_test_dataset,\n",
    "    compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90ea309c-5a3a-4331-906c-2e06c0b525b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 07:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.420229</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.412532</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.400505</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.4149353809845753, metrics={'train_runtime': 477.6473, 'train_samples_per_second': 0.628, 'train_steps_per_second': 0.082, 'total_flos': 21366375321600.0, 'train_loss': 1.4149353809845753, 'epoch': 3.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39314890-080a-4964-959f-23aea092b2d4",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ffddb79-7235-456a-ab68-73e242043db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4005045890808105,\n",
       " 'eval_accuracy': 0.22,\n",
       " 'eval_runtime': 37.6846,\n",
       " 'eval_samples_per_second': 2.654,\n",
       " 'eval_steps_per_second': 0.345,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c6d865c-e34c-443a-93d5-9eb62fe52c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2091de34-7a3c-447c-9f0c-45038fc206cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = XLNetForSequenceClassification.from_pretrained('fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e4779336-511c-4f02-b3cd-32f8a3eb3a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "clf = pipeline(\"text-classification\", fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd83e35c-e7fd-4ab5-8727-ca6f87d9c442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jw/c29_qvvs3yb90jqlkdbpt4_m0000gn/T/ipykernel_15618/1034133212.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(val_split['text_clean'][rand_int])\n",
      "/var/folders/jw/c29_qvvs3yb90jqlkdbpt4_m0000gn/T/ipykernel_15618/1034133212.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  answer = clf(val_split['text_clean'][rand_int], top_k = None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i wonder how a guy can broke his penis while having sex? #serious\n",
      "[{'label': 'sadness', 'score': 0.30957669019699097}, {'label': 'anger', 'score': 0.2612883150577545}, {'label': 'joy', 'score': 0.21543504297733307}, {'label': 'fear', 'score': 0.21369995176792145}]\n"
     ]
    }
   ],
   "source": [
    "rand_int = random.randint(0, len(val_split))\n",
    "print(val_split['text_clean'][rand_int])\n",
    "answer = clf(val_split['text_clean'][rand_int], top_k = None)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef23e1-ff15-47b2-958e-f53839c16b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_course_env",
   "language": "python",
   "name": "llms_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
